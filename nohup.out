INFO 02-19 17:29:41 __init__.py:190] Automatically detected platform cuda.
INFO 02-19 17:29:42 api_server.py:840] vLLM API server version 0.7.2
INFO 02-19 17:29:42 api_server.py:841] args: Namespace(subparser='serve', model_tag='snunlp/bigdata_gemma2_9b_dora', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='token-snuintern2025', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='snunlp/bigdata_gemma2_9b_dora', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f50b34589a0>)
INFO 02-19 17:29:42 api_server.py:206] Started engine process with PID 263629
INFO 02-19 17:29:47 __init__.py:190] Automatically detected platform cuda.
INFO 02-19 17:29:52 config.py:542] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.
INFO 02-19 17:29:52 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-19 17:29:57 config.py:542] This model supports multiple tasks: {'score', 'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 02-19 17:29:58 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-19 17:29:58 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='snunlp/bigdata_gemma2_9b_dora', speculative_config=None, tokenizer='snunlp/bigdata_gemma2_9b_dora', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=snunlp/bigdata_gemma2_9b_dora, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
WARNING 02-19 17:29:59 multiproc_worker_utils.py:300] Reducing Torch parallelism from 40 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-19 17:29:59 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 02-19 17:30:00 cuda.py:230] Using Flash Attention backend.
INFO 02-19 17:30:03 __init__.py:190] Automatically detected platform cuda.
INFO 02-19 17:30:03 __init__.py:190] Automatically detected platform cuda.
INFO 02-19 17:30:03 __init__.py:190] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=264286)[0;0m INFO 02-19 17:30:04 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=264287)[0;0m INFO 02-19 17:30:05 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=264285)[0;0m INFO 02-19 17:30:05 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=264286)[0;0m INFO 02-19 17:30:05 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=264287)[0;0m INFO 02-19 17:30:06 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=264285)[0;0m INFO 02-19 17:30:06 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=264286)[0;0m INFO 02-19 17:30:07 utils.py:950] Found nccl from library libnccl.so.2
INFO 02-19 17:30:07 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=264286)[0;0m INFO 02-19 17:30:07 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=264287)[0;0m INFO 02-19 17:30:07 utils.py:950] Found nccl from library libnccl.so.2
INFO 02-19 17:30:07 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=264287)[0;0m INFO 02-19 17:30:07 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=264285)[0;0m INFO 02-19 17:30:07 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=264285)[0;0m INFO 02-19 17:30:07 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=264287)[0;0m WARNING 02-19 17:30:08 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=264286)[0;0m WARNING 02-19 17:30:08 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=264285)[0;0m WARNING 02-19 17:30:08 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 02-19 17:30:08 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 02-19 17:30:08 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_d5d40e19'), local_subscribe_port=59147, remote_subscribe_port=None)
INFO 02-19 17:30:08 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=264285)[0;0m INFO 02-19 17:30:08 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=264287)[0;0m INFO 02-19 17:30:08 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=264286)[0;0m INFO 02-19 17:30:08 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 383, in load_model
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 414, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = Gemma2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 263, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 558, in make_layers
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 265, in <lambda>
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     lambda prefix: Gemma2DecoderLayer(
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 203, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.mlp = Gemma2MLP(
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                ^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 69, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.down_proj = RowParallelLinear(intermediate_size,
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1054, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 1 has a total capacity of 39.39 GiB of which 9.31 MiB is free. Process 262113 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 383, in load_model
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 414, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = Gemma2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 263, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 558, in make_layers
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 265, in <lambda>
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     lambda prefix: Gemma2DecoderLayer(
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 203, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.mlp = Gemma2MLP(
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                ^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 69, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.down_proj = RowParallelLinear(intermediate_size,
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1054, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 3 has a total capacity of 39.39 GiB of which 9.31 MiB is free. Process 262115 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 383, in load_model
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 414, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = Gemma2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 263, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 558, in make_layers
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 265, in <lambda>
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     lambda prefix: Gemma2DecoderLayer(
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 203, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.mlp = Gemma2MLP(
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                ^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 69, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.down_proj = RowParallelLinear(intermediate_size,
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1054, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 2 has a total capacity of 39.39 GiB of which 9.31 MiB is free. Process 262114 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 02-19 17:30:08 engine.py:389] CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 7.31 MiB is free. Process 261785 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 02-19 17:30:08 engine.py:389] Traceback (most recent call last):
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 380, in run_mp_engine
ERROR 02-19 17:30:08 engine.py:389]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,
ERROR 02-19 17:30:08 engine.py:389]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 123, in from_engine_args
ERROR 02-19 17:30:08 engine.py:389]     return cls(ipc_path=ipc_path,
ERROR 02-19 17:30:08 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 75, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.engine = LLMEngine(*args, **kwargs)
ERROR 02-19 17:30:08 engine.py:389]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 273, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.model_executor = executor_class(vllm_config=vllm_config, )
ERROR 02-19 17:30:08 engine.py:389]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 262, in __init__
ERROR 02-19 17:30:08 engine.py:389]     super().__init__(*args, **kwargs)
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 51, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self._init_executor()
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 125, in _init_executor
ERROR 02-19 17:30:08 engine.py:389]     self._run_workers("load_model",
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 185, in _run_workers
ERROR 02-19 17:30:08 engine.py:389]     driver_worker_output = run_method(self.driver_worker, sent_method,
ERROR 02-19 17:30:08 engine.py:389]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
ERROR 02-19 17:30:08 engine.py:389]     return func(*args, **kwargs)
ERROR 02-19 17:30:08 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 02-19 17:30:08 engine.py:389]     self.model_runner.load_model()
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
ERROR 02-19 17:30:08 engine.py:389]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 02-19 17:30:08 engine.py:389]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 02-19 17:30:08 engine.py:389]     return loader.load_model(vllm_config=vllm_config)
ERROR 02-19 17:30:08 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 383, in load_model
ERROR 02-19 17:30:08 engine.py:389]     model = _initialize_model(vllm_config=vllm_config)
ERROR 02-19 17:30:08 engine.py:389]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
ERROR 02-19 17:30:08 engine.py:389]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 02-19 17:30:08 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 414, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.model = Gemma2Model(vllm_config=vllm_config,
ERROR 02-19 17:30:08 engine.py:389]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 02-19 17:30:08 engine.py:389]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 263, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 02-19 17:30:08 engine.py:389]                                                     ^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 558, in make_layers
ERROR 02-19 17:30:08 engine.py:389]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 02-19 17:30:08 engine.py:389]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 265, in <lambda>
ERROR 02-19 17:30:08 engine.py:389]     lambda prefix: Gemma2DecoderLayer(
ERROR 02-19 17:30:08 engine.py:389]                    ^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 203, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.mlp = Gemma2MLP(
ERROR 02-19 17:30:08 engine.py:389]                ^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 69, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.down_proj = RowParallelLinear(intermediate_size,
ERROR 02-19 17:30:08 engine.py:389]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1054, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.quant_method.create_weights(
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
ERROR 02-19 17:30:08 engine.py:389]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 02-19 17:30:08 engine.py:389]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 106, in __torch_function__
ERROR 02-19 17:30:08 engine.py:389]     return func(*args, **kwargs)
ERROR 02-19 17:30:08 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 7.31 MiB is free. Process 261785 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 02-19 17:30:09 multiproc_worker_utils.py:124] Worker VllmWorkerProcess pid 264287 died, exit code: -15
Process SpawnProcess-1:
INFO 02-19 17:30:09 multiproc_worker_utils.py:128] Killing local vLLM worker processes
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/anaconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 391, in run_mp_engine
    raise e
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 380, in run_mp_engine
    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 123, in from_engine_args
    return cls(ipc_path=ipc_path,
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 75, in __init__
    self.engine = LLMEngine(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 273, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config, )
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 262, in __init__
    super().__init__(*args, **kwargs)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 51, in __init__
    self._init_executor()
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 125, in _init_executor
    self._run_workers("load_model",
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 185, in _run_workers
    driver_worker_output = run_method(self.driver_worker, sent_method,
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 383, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 414, in __init__
    self.model = Gemma2Model(vllm_config=vllm_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 263, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
                                                    ^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 558, in make_layers
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 265, in <lambda>
    lambda prefix: Gemma2DecoderLayer(
                   ^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 203, in __init__
    self.mlp = Gemma2MLP(
               ^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 69, in __init__
    self.down_proj = RowParallelLinear(intermediate_size,
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1054, in __init__
    self.quant_method.create_weights(
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 106, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 7.31 MiB is free. Process 261785 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W219 17:30:10.084460741 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Traceback (most recent call last):
  File "/home/hyeznee/.local/bin/vllm", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 204, in main
    args.dispatch_function(args)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 44, in serve
    uvloop.run(run_server(args))
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 875, in run_server
    async with build_async_engine_client(args) as engine_client:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 136, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 230, in build_async_engine_client_from_engine_args
    raise RuntimeError(
RuntimeError: Engine process failed to start. See stack trace for the root cause.
/opt/anaconda3/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
