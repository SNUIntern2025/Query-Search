INFO 02-19 17:29:41 __init__.py:190] Automatically detected platform cuda.
INFO 02-19 17:29:42 api_server.py:840] vLLM API server version 0.7.2
INFO 02-19 17:29:42 api_server.py:841] args: Namespace(subparser='serve', model_tag='snunlp/bigdata_gemma2_9b_dora', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='token-snuintern2025', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='snunlp/bigdata_gemma2_9b_dora', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f50b34589a0>)
INFO 02-19 17:29:42 api_server.py:206] Started engine process with PID 263629
INFO 02-19 17:29:47 __init__.py:190] Automatically detected platform cuda.
INFO 02-19 17:29:52 config.py:542] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.
INFO 02-19 17:29:52 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-19 17:29:57 config.py:542] This model supports multiple tasks: {'score', 'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 02-19 17:29:58 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-19 17:29:58 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='snunlp/bigdata_gemma2_9b_dora', speculative_config=None, tokenizer='snunlp/bigdata_gemma2_9b_dora', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=snunlp/bigdata_gemma2_9b_dora, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
WARNING 02-19 17:29:59 multiproc_worker_utils.py:300] Reducing Torch parallelism from 40 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-19 17:29:59 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 02-19 17:30:00 cuda.py:230] Using Flash Attention backend.
INFO 02-19 17:30:03 __init__.py:190] Automatically detected platform cuda.
INFO 02-19 17:30:03 __init__.py:190] Automatically detected platform cuda.
INFO 02-19 17:30:03 __init__.py:190] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=264286)[0;0m INFO 02-19 17:30:04 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=264287)[0;0m INFO 02-19 17:30:05 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=264285)[0;0m INFO 02-19 17:30:05 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=264286)[0;0m INFO 02-19 17:30:05 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=264287)[0;0m INFO 02-19 17:30:06 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=264285)[0;0m INFO 02-19 17:30:06 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=264286)[0;0m INFO 02-19 17:30:07 utils.py:950] Found nccl from library libnccl.so.2
INFO 02-19 17:30:07 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=264286)[0;0m INFO 02-19 17:30:07 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=264287)[0;0m INFO 02-19 17:30:07 utils.py:950] Found nccl from library libnccl.so.2
INFO 02-19 17:30:07 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=264287)[0;0m INFO 02-19 17:30:07 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=264285)[0;0m INFO 02-19 17:30:07 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=264285)[0;0m INFO 02-19 17:30:07 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=264287)[0;0m WARNING 02-19 17:30:08 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=264286)[0;0m WARNING 02-19 17:30:08 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=264285)[0;0m WARNING 02-19 17:30:08 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 02-19 17:30:08 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 02-19 17:30:08 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_d5d40e19'), local_subscribe_port=59147, remote_subscribe_port=None)
INFO 02-19 17:30:08 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=264285)[0;0m INFO 02-19 17:30:08 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=264287)[0;0m INFO 02-19 17:30:08 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=264286)[0;0m INFO 02-19 17:30:08 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 383, in load_model
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 414, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = Gemma2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 263, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 558, in make_layers
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 265, in <lambda>
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     lambda prefix: Gemma2DecoderLayer(
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 203, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.mlp = Gemma2MLP(
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                ^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 69, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.down_proj = RowParallelLinear(intermediate_size,
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1054, in __init__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264285)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 1 has a total capacity of 39.39 GiB of which 9.31 MiB is free. Process 262113 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 383, in load_model
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 414, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = Gemma2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 263, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 558, in make_layers
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 265, in <lambda>
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     lambda prefix: Gemma2DecoderLayer(
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 203, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.mlp = Gemma2MLP(
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                ^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 69, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.down_proj = RowParallelLinear(intermediate_size,
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1054, in __init__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264287)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 3 has a total capacity of 39.39 GiB of which 9.31 MiB is free. Process 262115 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 183, in load_model
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model_runner.load_model()
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 383, in load_model
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     model = _initialize_model(vllm_config=vllm_config)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return model_class(vllm_config=vllm_config, prefix=prefix)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 414, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.model = Gemma2Model(vllm_config=vllm_config,
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 263, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.start_layer, self.end_layer, self.layers = make_layers(
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                                                     ^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 558, in make_layers
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 265, in <lambda>
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     lambda prefix: Gemma2DecoderLayer(
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                    ^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 203, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.mlp = Gemma2MLP(
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                ^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 69, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.down_proj = RowParallelLinear(intermediate_size,
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1054, in __init__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     self.quant_method.create_weights(
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     weight = Parameter(torch.empty(sum(output_partition_sizes),
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=264286)[0;0m ERROR 02-19 17:30:08 multiproc_worker_utils.py:242] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 2 has a total capacity of 39.39 GiB of which 9.31 MiB is free. Process 262114 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 02-19 17:30:08 engine.py:389] CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 7.31 MiB is free. Process 261785 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 02-19 17:30:08 engine.py:389] Traceback (most recent call last):
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 380, in run_mp_engine
ERROR 02-19 17:30:08 engine.py:389]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,
ERROR 02-19 17:30:08 engine.py:389]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 123, in from_engine_args
ERROR 02-19 17:30:08 engine.py:389]     return cls(ipc_path=ipc_path,
ERROR 02-19 17:30:08 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 75, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.engine = LLMEngine(*args, **kwargs)
ERROR 02-19 17:30:08 engine.py:389]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 273, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.model_executor = executor_class(vllm_config=vllm_config, )
ERROR 02-19 17:30:08 engine.py:389]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 262, in __init__
ERROR 02-19 17:30:08 engine.py:389]     super().__init__(*args, **kwargs)
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 51, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self._init_executor()
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 125, in _init_executor
ERROR 02-19 17:30:08 engine.py:389]     self._run_workers("load_model",
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 185, in _run_workers
ERROR 02-19 17:30:08 engine.py:389]     driver_worker_output = run_method(self.driver_worker, sent_method,
ERROR 02-19 17:30:08 engine.py:389]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
ERROR 02-19 17:30:08 engine.py:389]     return func(*args, **kwargs)
ERROR 02-19 17:30:08 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 183, in load_model
ERROR 02-19 17:30:08 engine.py:389]     self.model_runner.load_model()
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
ERROR 02-19 17:30:08 engine.py:389]     self.model = get_model(vllm_config=self.vllm_config)
ERROR 02-19 17:30:08 engine.py:389]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
ERROR 02-19 17:30:08 engine.py:389]     return loader.load_model(vllm_config=vllm_config)
ERROR 02-19 17:30:08 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 383, in load_model
ERROR 02-19 17:30:08 engine.py:389]     model = _initialize_model(vllm_config=vllm_config)
ERROR 02-19 17:30:08 engine.py:389]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
ERROR 02-19 17:30:08 engine.py:389]     return model_class(vllm_config=vllm_config, prefix=prefix)
ERROR 02-19 17:30:08 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 414, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.model = Gemma2Model(vllm_config=vllm_config,
ERROR 02-19 17:30:08 engine.py:389]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
ERROR 02-19 17:30:08 engine.py:389]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 263, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.start_layer, self.end_layer, self.layers = make_layers(
ERROR 02-19 17:30:08 engine.py:389]                                                     ^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 558, in make_layers
ERROR 02-19 17:30:08 engine.py:389]     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
ERROR 02-19 17:30:08 engine.py:389]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 265, in <lambda>
ERROR 02-19 17:30:08 engine.py:389]     lambda prefix: Gemma2DecoderLayer(
ERROR 02-19 17:30:08 engine.py:389]                    ^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 203, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.mlp = Gemma2MLP(
ERROR 02-19 17:30:08 engine.py:389]                ^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 69, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.down_proj = RowParallelLinear(intermediate_size,
ERROR 02-19 17:30:08 engine.py:389]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1054, in __init__
ERROR 02-19 17:30:08 engine.py:389]     self.quant_method.create_weights(
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
ERROR 02-19 17:30:08 engine.py:389]     weight = Parameter(torch.empty(sum(output_partition_sizes),
ERROR 02-19 17:30:08 engine.py:389]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 106, in __torch_function__
ERROR 02-19 17:30:08 engine.py:389]     return func(*args, **kwargs)
ERROR 02-19 17:30:08 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 02-19 17:30:08 engine.py:389] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 7.31 MiB is free. Process 261785 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 02-19 17:30:09 multiproc_worker_utils.py:124] Worker VllmWorkerProcess pid 264287 died, exit code: -15
Process SpawnProcess-1:
INFO 02-19 17:30:09 multiproc_worker_utils.py:128] Killing local vLLM worker processes
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/anaconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 391, in run_mp_engine
    raise e
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 380, in run_mp_engine
    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 123, in from_engine_args
    return cls(ipc_path=ipc_path,
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 75, in __init__
    self.engine = LLMEngine(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 273, in __init__
    self.model_executor = executor_class(vllm_config=vllm_config, )
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 262, in __init__
    super().__init__(*args, **kwargs)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 51, in __init__
    self._init_executor()
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 125, in _init_executor
    self._run_workers("load_model",
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 185, in _run_workers
    driver_worker_output = run_method(self.driver_worker, sent_method,
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 183, in load_model
    self.model_runner.load_model()
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1112, in load_model
    self.model = get_model(vllm_config=self.vllm_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
    return loader.load_model(vllm_config=vllm_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 383, in load_model
    model = _initialize_model(vllm_config=vllm_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 414, in __init__
    self.model = Gemma2Model(vllm_config=vllm_config,
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 151, in __init__
    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 263, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
                                                    ^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 558, in make_layers
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 265, in <lambda>
    lambda prefix: Gemma2DecoderLayer(
                   ^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 203, in __init__
    self.mlp = Gemma2MLP(
               ^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 69, in __init__
    self.down_proj = RowParallelLinear(intermediate_size,
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 1054, in __init__
    self.quant_method.create_weights(
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 129, in create_weights
    weight = Parameter(torch.empty(sum(output_partition_sizes),
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_device.py", line 106, in __torch_function__
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 7.31 MiB is free. Process 261785 has 35.42 GiB memory in use. Including non-PyTorch memory, this process has 3.93 GiB memory in use. Of the allocated memory 3.21 GiB is allocated by PyTorch, and 128.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W219 17:30:10.084460741 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Traceback (most recent call last):
  File "/home/hyeznee/.local/bin/vllm", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 204, in main
    args.dispatch_function(args)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 44, in serve
    uvloop.run(run_server(args))
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 875, in run_server
    async with build_async_engine_client(args) as engine_client:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 136, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 230, in build_async_engine_client_from_engine_args
    raise RuntimeError(
RuntimeError: Engine process failed to start. See stack trace for the root cause.
/opt/anaconda3/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 02-19 23:38:07 __init__.py:190] Automatically detected platform cuda.
INFO 02-19 23:38:08 api_server.py:840] vLLM API server version 0.7.2
INFO 02-19 23:38:08 api_server.py:841] args: Namespace(subparser='serve', model_tag='snunlp/bigdata_gemma2_9b_dora', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='token-snuintern2025', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='snunlp/bigdata_gemma2_9b_dora', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f522938c9a0>)
INFO 02-19 23:38:08 api_server.py:206] Started engine process with PID 299583
INFO 02-19 23:38:12 __init__.py:190] Automatically detected platform cuda.
INFO 02-19 23:38:23 config.py:542] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 02-19 23:38:23 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-19 23:38:23 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='snunlp/bigdata_gemma2_9b_dora', speculative_config=None, tokenizer='snunlp/bigdata_gemma2_9b_dora', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=snunlp/bigdata_gemma2_9b_dora, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
WARNING 02-19 23:38:25 multiproc_worker_utils.py:300] Reducing Torch parallelism from 40 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-19 23:38:25 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 02-19 23:38:26 cuda.py:230] Using Flash Attention backend.
INFO 02-19 23:38:28 config.py:542] This model supports multiple tasks: {'classify', 'embed', 'score', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 02-19 23:38:28 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-19 23:38:28 __init__.py:190] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:29 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:30 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:31 utils.py:950] Found nccl from library libnccl.so.2
INFO 02-19 23:38:31 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:31 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-19 23:38:31 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:31 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/hyeznee/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 02-19 23:38:31 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/hyeznee/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 02-19 23:38:31 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_be452e9e'), local_subscribe_port=41093, remote_subscribe_port=None)
INFO 02-19 23:38:31 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:31 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
INFO 02-19 23:38:32 weight_utils.py:252] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:32 weight_utils.py:252] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.02it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.14s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.08s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.17s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.14s/it]

INFO 02-19 23:38:37 model_runner.py:1115] Loading model weights took 8.6536 GB
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:38 model_runner.py:1115] Loading model weights took 8.6536 GB
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:43 worker.py:267] Memory profiling takes 5.21 seconds
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:43 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:43 worker.py:267] model weights take 8.65GiB; non_torch_memory takes 0.30GiB; PyTorch activation peak memory takes 0.66GiB; the rest of the memory reserved for KV Cache is 25.84GiB.
INFO 02-19 23:38:43 worker.py:267] Memory profiling takes 5.27 seconds
INFO 02-19 23:38:43 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB
INFO 02-19 23:38:43 worker.py:267] model weights take 8.65GiB; non_torch_memory takes 0.30GiB; PyTorch activation peak memory takes 2.38GiB; the rest of the memory reserved for KV Cache is 24.12GiB.
INFO 02-19 23:38:44 executor_base.py:110] # CUDA blocks: 9409, # CPU blocks: 1560
INFO 02-19 23:38:44 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 18.38x
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:38:47 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-19 23:38:47 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:01<00:44,  1.30s/it]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:30,  1.08it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:26,  1.23it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:03<00:23,  1.30it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:04<00:22,  1.35it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:04<00:20,  1.39it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:05<00:19,  1.42it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:06<00:18,  1.43it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:06<00:18,  1.44it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:07<00:17,  1.46it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:08<00:16,  1.44it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:08<00:15,  1.46it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:09<00:15,  1.45it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:10<00:14,  1.46it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:10<00:13,  1.47it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:11<00:12,  1.47it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:12<00:12,  1.44it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:12<00:11,  1.45it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:13<00:11,  1.45it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:14<00:10,  1.46it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:14<00:09,  1.47it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:15<00:08,  1.47it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:16<00:08,  1.47it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:17<00:07,  1.44it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:17<00:06,  1.45it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:18<00:06,  1.45it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:19<00:05,  1.46it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:19<00:04,  1.47it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:20<00:04,  1.48it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:21<00:03,  1.48it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:21<00:02,  1.48it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:22<00:02,  1.45it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:23<00:01,  1.44it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:23<00:00,  1.45it/s][1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:39:11 custom_all_reduce.py:226] Registering 2975 cuda graph addresses
Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:25<00:00,  1.06s/it]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:25<00:00,  1.36it/s]
INFO 02-19 23:39:12 custom_all_reduce.py:226] Registering 2975 cuda graph addresses
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-19 23:39:12 model_runner.py:1562] Graph capturing finished in 26 secs, took 1.49 GiB
INFO 02-19 23:39:12 model_runner.py:1562] Graph capturing finished in 26 secs, took 1.49 GiB
INFO 02-19 23:39:12 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 34.66 seconds
INFO 02-19 23:39:13 api_server.py:756] Using supplied chat template:
INFO 02-19 23:39:13 api_server.py:756] None
INFO 02-19 23:39:13 launcher.py:21] Available routes are:
INFO 02-19 23:39:13 launcher.py:29] Route: /openapi.json, Methods: GET, HEAD
INFO 02-19 23:39:13 launcher.py:29] Route: /docs, Methods: GET, HEAD
INFO 02-19 23:39:13 launcher.py:29] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 02-19 23:39:13 launcher.py:29] Route: /redoc, Methods: GET, HEAD
INFO 02-19 23:39:13 launcher.py:29] Route: /health, Methods: GET
INFO 02-19 23:39:13 launcher.py:29] Route: /ping, Methods: POST, GET
INFO 02-19 23:39:13 launcher.py:29] Route: /tokenize, Methods: POST
INFO 02-19 23:39:13 launcher.py:29] Route: /detokenize, Methods: POST
INFO 02-19 23:39:13 launcher.py:29] Route: /v1/models, Methods: GET
INFO 02-19 23:39:13 launcher.py:29] Route: /version, Methods: GET
INFO 02-19 23:39:13 launcher.py:29] Route: /v1/chat/completions, Methods: POST
INFO 02-19 23:39:13 launcher.py:29] Route: /v1/completions, Methods: POST
INFO 02-19 23:39:13 launcher.py:29] Route: /v1/embeddings, Methods: POST
INFO 02-19 23:39:13 launcher.py:29] Route: /pooling, Methods: POST
INFO 02-19 23:39:13 launcher.py:29] Route: /score, Methods: POST
INFO 02-19 23:39:13 launcher.py:29] Route: /v1/score, Methods: POST
INFO 02-19 23:39:13 launcher.py:29] Route: /rerank, Methods: POST
INFO 02-19 23:39:13 launcher.py:29] Route: /v1/rerank, Methods: POST
INFO 02-19 23:39:13 launcher.py:29] Route: /v2/rerank, Methods: POST
INFO 02-19 23:39:13 launcher.py:29] Route: /invocations, Methods: POST
INFO:     Started server process [299415]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO 02-20 00:01:57 launcher.py:59] Shutting down FastAPI HTTP server.
INFO 02-20 00:01:57 multiproc_worker_utils.py:141] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=300150)[0;0m INFO 02-20 00:01:57 multiproc_worker_utils.py:253] Worker exiting
[rank0]:[W220 00:01:59.588688914 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
/opt/anaconda3/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 02-20 01:01:40 __init__.py:190] Automatically detected platform cuda.
INFO 02-20 01:01:41 api_server.py:840] vLLM API server version 0.7.2
INFO 02-20 01:01:41 api_server.py:841] args: Namespace(subparser='serve', model_tag='snunlp/bigdata_gemma2_9b_dora', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='token-snuintern2025', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='snunlp/bigdata_gemma2_9b_dora', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f5997e80860>)
INFO 02-20 01:01:41 api_server.py:206] Started engine process with PID 311009
INFO 02-20 01:01:46 __init__.py:190] Automatically detected platform cuda.
INFO 02-20 01:01:51 config.py:542] This model supports multiple tasks: {'classify', 'generate', 'embed', 'score', 'reward'}. Defaulting to 'generate'.
INFO 02-20 01:01:51 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-20 01:01:52 __init__.py:190] Automatically detected platform cuda.
INFO 02-20 01:01:53 api_server.py:840] vLLM API server version 0.7.2
INFO 02-20 01:01:53 api_server.py:841] args: Namespace(subparser='serve', model_tag='snunlp/bigdata_gemma2_9b_dora', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='token-snuintern2025', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='snunlp/bigdata_gemma2_9b_dora', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f184988c860>)
INFO 02-20 01:01:53 api_server.py:206] Started engine process with PID 311581
INFO 02-20 01:01:57 config.py:542] This model supports multiple tasks: {'reward', 'classify', 'score', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 02-20 01:01:57 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-20 01:01:57 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='snunlp/bigdata_gemma2_9b_dora', speculative_config=None, tokenizer='snunlp/bigdata_gemma2_9b_dora', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=snunlp/bigdata_gemma2_9b_dora, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
INFO 02-20 01:01:58 __init__.py:190] Automatically detected platform cuda.
WARNING 02-20 01:01:58 multiproc_worker_utils.py:300] Reducing Torch parallelism from 40 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-20 01:01:58 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 02-20 01:01:59 cuda.py:230] Using Flash Attention backend.
INFO 02-20 01:02:02 __init__.py:190] Automatically detected platform cuda.
INFO 02-20 01:02:02 __init__.py:190] Automatically detected platform cuda.
INFO 02-20 01:02:03 api_server.py:840] vLLM API server version 0.7.2
INFO 02-20 01:02:03 api_server.py:841] args: Namespace(subparser='serve', model_tag='snunlp/bigdata_gemma2_9b_dora', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='token-snuintern2025', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='snunlp/bigdata_gemma2_9b_dora', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7faf0ae70860>)
INFO 02-20 01:02:03 api_server.py:206] Started engine process with PID 312017
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:03 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:04 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:05 utils.py:950] Found nccl from library libnccl.so.2
INFO 02-20 01:02:05 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:05 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-20 01:02:05 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:05 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/hyeznee/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 02-20 01:02:05 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/hyeznee/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 02-20 01:02:05 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_cbdae919'), local_subscribe_port=35097, remote_subscribe_port=None)
INFO 02-20 01:02:05 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:05 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py", line 122, in spawn_main
    exitcode = _main(fd, parent_sentinel)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py", line 131, in _main
    prepare(preparation_data)
  File "/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py", line 246, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File "/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py", line 297, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen runpy>", line 287, in run_path
  File "<frozen runpy>", line 98, in _run_module_code
  File "<frozen runpy>", line 88, in _run_code
  File "/home/hyeznee/.local/bin/vllm", line 5, in <module>
    from vllm.scripts import main
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/__init__.py", line 7, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 13, in <module>
    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/config.py", line 24, in <module>
    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/__init__.py", line 3, in <module>
    from vllm.model_executor.parameter import (BasevLLMParameter,
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/parameter.py", line 9, in <module>
    from vllm.distributed import get_tensor_model_parallel_rank
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/distributed/__init__.py", line 3, in <module>
    from .communication_op import *
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/distributed/communication_op.py", line 8, in <module>
    from .parallel_state import get_tp_group
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/distributed/parallel_state.py", line 40, in <module>
    import vllm.distributed.kv_transfer.kv_transfer_agent as kv_transfer
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/distributed/kv_transfer/kv_transfer_agent.py", line 16, in <module>
    from vllm.distributed.kv_transfer.kv_connector.factory import (
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/distributed/kv_transfer/kv_connector/factory.py", line 6, in <module>
    from .base import KVConnectorBase
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/distributed/kv_transfer/kv_connector/base.py", line 15, in <module>
    from vllm.sequence import IntermediateTensors
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/sequence.py", line 17, in <module>
    from vllm.inputs import SingletonInputs, SingletonInputsAdapter
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/inputs/__init__.py", line 9, in <module>
    from .registry import (DummyData, InputContext, InputProcessingContext,
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/inputs/registry.py", line 10, in <module>
    from transformers import BatchFeature, PretrainedConfig, ProcessorMixin
  File "<frozen importlib._bootstrap>", line 1412, in _handle_fromlist
  File "/home/hyeznee/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py", line 1805, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/transformers/utils/import_utils.py", line 1817, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/importlib/__init__.py", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/transformers/processing_utils.py", line 33, in <module>
    from .image_utils import ChannelDimension, is_valid_image, is_vision_available
  File "/home/hyeznee/.local/lib/python3.12/site-packages/transformers/image_utils.py", line 59, in <module>
    from torchvision.transforms import InterpolationMode
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torchvision/__init__.py", line 10, in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torchvision/models/__init__.py", line 2, in <module>
    from .convnext import *
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torchvision/models/convnext.py", line 8, in <module>
    from ..ops.misc import Conv2dNormActivation, Permute
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torchvision/ops/__init__.py", line 23, in <module>
ERROR 02-20 01:02:06 registry.py:306] Error in inspecting model architecture 'Gemma2ForCausalLM'
ERROR 02-20 01:02:06 registry.py:306] Traceback (most recent call last):
ERROR 02-20 01:02:06 registry.py:306]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 507, in _run_in_subprocess
ERROR 02-20 01:02:06 registry.py:306]     returned.check_returncode()
ERROR 02-20 01:02:06 registry.py:306]   File "/opt/anaconda3/lib/python3.12/subprocess.py", line 502, in check_returncode
ERROR 02-20 01:02:06 registry.py:306]     raise CalledProcessError(self.returncode, self.args, self.stdout,
ERROR 02-20 01:02:06 registry.py:306] subprocess.CalledProcessError: Command '['/opt/anaconda3/bin/python', '-m', 'vllm.model_executor.models.registry']' died with <Signals.SIGINT: 2>.
ERROR 02-20 01:02:06 registry.py:306] 
ERROR 02-20 01:02:06 registry.py:306] The above exception was the direct cause of the following exception:
ERROR 02-20 01:02:06 registry.py:306] 
ERROR 02-20 01:02:06 registry.py:306] Traceback (most recent call last):
ERROR 02-20 01:02:06 registry.py:306]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 304, in _try_inspect_model_cls
ERROR 02-20 01:02:06 registry.py:306]     return model.inspect_model_cls()
ERROR 02-20 01:02:06 registry.py:306]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:06 registry.py:306]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 275, in inspect_model_cls
ERROR 02-20 01:02:06 registry.py:306]     return _run_in_subprocess(
ERROR 02-20 01:02:06 registry.py:306]            ^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:06 registry.py:306]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 510, in _run_in_subprocess
ERROR 02-20 01:02:06 registry.py:306]     raise RuntimeError(f"Error raised in subprocess:\n"
ERROR 02-20 01:02:06 registry.py:306] RuntimeError: Error raised in subprocess:
ERROR 02-20 01:02:06 registry.py:306] Traceback (most recent call last):
ERROR 02-20 01:02:06 registry.py:306]   File "<frozen runpy>", line 189, in _run_module_as_main
ERROR 02-20 01:02:06 registry.py:306]   File "<frozen runpy>", line 112, in _get_module_details
ERROR 02-20 01:02:06 registry.py:306]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/__init__.py", line 5, in <module>
ERROR 02-20 01:02:06 registry.py:306]     import torch
ERROR 02-20 01:02:06 registry.py:306]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/__init__.py", line 1954, in <module>
ERROR 02-20 01:02:06 registry.py:306]     _C._initExtension(_manager_path())
ERROR 02-20 01:02:06 registry.py:306]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/cuda/__init__.py", line 61, in <module>
ERROR 02-20 01:02:06 registry.py:306]     import pynvml  # type: ignore[import]
ERROR 02-20 01:02:06 registry.py:306]     ^^^^^^^^^^^^^
ERROR 02-20 01:02:06 registry.py:306]   File "/home/hyeznee/.local/lib/python3.12/site-packages/pynvml.py", line 1768, in <module>
ERROR 02-20 01:02:06 registry.py:306]     c_nvmlEventSet_t = POINTER(struct_c_nvmlEventSet_t)
ERROR 02-20 01:02:06 registry.py:306]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:06 registry.py:306] KeyboardInterrupt
ERROR 02-20 01:02:06 registry.py:306] 
Traceback (most recent call last):
  File "/home/hyeznee/.local/bin/vllm", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 204, in main
    args.dispatch_function(args)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 44, in serve
    uvloop.run(run_server(args))
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 875, in run_server
    async with build_async_engine_client(args) as engine_client:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 136, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 217, in build_async_engine_client_from_engine_args
    engine_config = engine_args.create_engine_config()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1075, in create_engine_config
    model_config = self.create_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 998, in create_model_config
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/config.py", line 364, in __init__
    self.multimodal_config = self._init_multimodal_config(
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/config.py", line 424, in _init_multimodal_config
    if ModelRegistry.is_multimodal_model(architectures):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 445, in is_multimodal_model
    model_cls, _ = self.inspect_model_cls(architectures)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 405, in inspect_model_cls
    return self._raise_for_unsupported(architectures)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 357, in _raise_for_unsupported
    raise ValueError(
ValueError: Model architectures ['Gemma2ForCausalLM'] failed to be inspected. Please check the logs for more details.
    from .poolers import MultiScaleRoIAlign
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torchvision/ops/poolers.py", line 10, in <module>
    from .roi_align import roi_align
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torchvision/ops/roi_align.py", line 7, in <module>
    from torch._dynamo.utils import is_compile_supported
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/_dynamo/__init__.py", line 39, in <module>
    from .polyfills import loader as _  # usort: skip # noqa: F401
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/_dynamo/polyfills/loader.py", line 22, in <module>
    POLYFILLED_MODULES: Tuple["ModuleType", ...] = tuple(
                                                   ^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/_dynamo/polyfills/loader.py", line 23, in <genexpr>
    importlib.import_module(f".{submodule}", package=polyfills.__name__)
  File "/opt/anaconda3/lib/python3.12/importlib/__init__.py", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/_dynamo/polyfills/builtins.py", line 23, in <module>
    @substitute_in_graph(builtins.all, can_constant_fold_through=True)
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/_dynamo/decorators.py", line 312, in wrapper
    rule_map: Dict[Any, Type[VariableTracker]] = get_torch_obj_rule_map()
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/_dynamo/trace_rules.py", line 2860, in get_torch_obj_rule_map
    obj = load_object(k)
          ^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/_dynamo/trace_rules.py", line 2891, in load_object
    val = _load_obj_from_str(x[0])
          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/_dynamo/trace_rules.py", line 2875, in _load_obj_from_str
    return getattr(importlib.import_module(module), obj_name)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/importlib/__init__.py", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1387, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1356, in _find_and_load
KeyboardInterrupt
INFO 02-20 01:02:06 weight_utils.py:252] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.03s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.16s/it]
INFO 02-20 01:02:08 config.py:542] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.
INFO 02-20 01:02:09 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-20 01:02:09 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='snunlp/bigdata_gemma2_9b_dora', speculative_config=None, tokenizer='snunlp/bigdata_gemma2_9b_dora', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=snunlp/bigdata_gemma2_9b_dora, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.11s/it]
WARNING 02-20 01:02:10 multiproc_worker_utils.py:300] Reducing Torch parallelism from 40 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-20 01:02:10 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.18s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]

INFO 02-20 01:02:11 cuda.py:230] Using Flash Attention backend.
INFO 02-20 01:02:11 model_runner.py:1115] Loading model weights took 8.6536 GB
INFO 02-20 01:02:13 config.py:542] This model supports multiple tasks: {'score', 'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 02-20 01:02:13 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-20 01:02:14 __init__.py:190] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=312479)[0;0m INFO 02-20 01:02:15 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
INFO 02-20 01:02:16 __init__.py:190] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:16 weight_utils.py:252] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=312479)[0;0m INFO 02-20 01:02:16 cuda.py:230] Using Flash Attention backend.
INFO 02-20 01:02:17 api_server.py:840] vLLM API server version 0.7.2
INFO 02-20 01:02:17 api_server.py:841] args: Namespace(subparser='serve', model_tag='snunlp/bigdata_gemma2_9b_dora', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='token-snuintern2025', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, enable_reasoning=False, reasoning_parser=None, tool_call_parser=None, tool_parser_plugin='', model='snunlp/bigdata_gemma2_9b_dora', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f9262a74860>)
INFO 02-20 01:02:17 api_server.py:206] Started engine process with PID 312652
[1;36m(VllmWorkerProcess pid=312479)[0;0m INFO 02-20 01:02:17 utils.py:950] Found nccl from library libnccl.so.2
INFO 02-20 01:02:17 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=312479)[0;0m INFO 02-20 01:02:17 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-20 01:02:17 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=312479)[0;0m INFO 02-20 01:02:17 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/hyeznee/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 02-20 01:02:17 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/hyeznee/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 02-20 01:02:18 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_386742b7'), local_subscribe_port=50023, remote_subscribe_port=None)
INFO 02-20 01:02:18 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=312479)[0;0m INFO 02-20 01:02:18 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
INFO 02-20 01:02:18 weight_utils.py:252] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=312479)[0;0m INFO 02-20 01:02:18 weight_utils.py:252] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.04s/it]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.19s/it]
INFO 02-20 01:02:21 __init__.py:190] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:21 model_runner.py:1115] Loading model weights took 8.6536 GB
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.12s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.20s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.17s/it]

INFO 02-20 01:02:24 model_runner.py:1115] Loading model weights took 8.6536 GB
[1;36m(VllmWorkerProcess pid=312479)[0;0m INFO 02-20 01:02:24 model_runner.py:1115] Loading model weights took 8.6536 GB
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:27 worker.py:267] Memory profiling takes 5.26 seconds
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:27 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:27 worker.py:267] model weights take 8.65GiB; non_torch_memory takes 9.64GiB; PyTorch activation peak memory takes 0.66GiB; the rest of the memory reserved for KV Cache is 16.49GiB.
INFO 02-20 01:02:27 config.py:542] This model supports multiple tasks: {'reward', 'embed', 'score', 'generate', 'classify'}. Defaulting to 'generate'.
INFO 02-20 01:02:27 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-20 01:02:27 worker.py:267] Memory profiling takes 5.38 seconds
INFO 02-20 01:02:27 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB
INFO 02-20 01:02:27 worker.py:267] model weights take 8.65GiB; non_torch_memory takes 9.64GiB; PyTorch activation peak memory takes 2.38GiB; the rest of the memory reserved for KV Cache is 14.77GiB.
INFO 02-20 01:02:27 executor_base.py:110] # CUDA blocks: 5763, # CPU blocks: 1560
INFO 02-20 01:02:27 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 11.26x
[1;36m(VllmWorkerProcess pid=312479)[0;0m INFO 02-20 01:02:29 worker.py:267] Memory profiling takes 5.14 seconds
[1;36m(VllmWorkerProcess pid=312479)[0;0m INFO 02-20 01:02:29 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB
[1;36m(VllmWorkerProcess pid=312479)[0;0m INFO 02-20 01:02:29 worker.py:267] model weights take 8.65GiB; non_torch_memory takes 15.26GiB; PyTorch activation peak memory takes 0.66GiB; the rest of the memory reserved for KV Cache is 10.87GiB.
INFO 02-20 01:02:30 worker.py:267] Memory profiling takes 5.22 seconds
INFO 02-20 01:02:30 worker.py:267] the current vLLM instance can use total_gpu_memory (39.39GiB) x gpu_memory_utilization (0.90) = 35.45GiB
INFO 02-20 01:02:30 worker.py:267] model weights take 8.65GiB; non_torch_memory takes 15.26GiB; PyTorch activation peak memory takes 2.38GiB; the rest of the memory reserved for KV Cache is 9.15GiB.
INFO 02-20 01:02:30 executor_base.py:110] # CUDA blocks: 3570, # CPU blocks: 1560
INFO 02-20 01:02:30 executor_base.py:115] Maximum concurrency for 8192 tokens per request: 6.97x
ERROR 02-20 01:02:30 engine.py:389] CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 29.25 MiB is free. Process 311009 has 24.24 GiB memory in use. Including non-PyTorch memory, this process has 15.08 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 26.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 02-20 01:02:30 engine.py:389] Traceback (most recent call last):
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 380, in run_mp_engine
ERROR 02-20 01:02:30 engine.py:389]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,
ERROR 02-20 01:02:30 engine.py:389]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 123, in from_engine_args
ERROR 02-20 01:02:30 engine.py:389]     return cls(ipc_path=ipc_path,
ERROR 02-20 01:02:30 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 75, in __init__
ERROR 02-20 01:02:30 engine.py:389]     self.engine = LLMEngine(*args, **kwargs)
ERROR 02-20 01:02:30 engine.py:389]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 276, in __init__
ERROR 02-20 01:02:30 engine.py:389]     self._initialize_kv_caches()
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 429, in _initialize_kv_caches
ERROR 02-20 01:02:30 engine.py:389]     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 121, in initialize_cache
ERROR 02-20 01:02:30 engine.py:389]     self.collective_rpc("initialize_cache",
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 307, in collective_rpc
ERROR 02-20 01:02:30 engine.py:389]     return self._run_workers(method, *args, **(kwargs or {}))
ERROR 02-20 01:02:30 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 185, in _run_workers
ERROR 02-20 01:02:30 engine.py:389]     driver_worker_output = run_method(self.driver_worker, sent_method,
ERROR 02-20 01:02:30 engine.py:389]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
ERROR 02-20 01:02:30 engine.py:389]     return func(*args, **kwargs)
ERROR 02-20 01:02:30 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 306, in initialize_cache
ERROR 02-20 01:02:30 engine.py:389]     self._init_cache_engine()
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 312, in _init_cache_engine
ERROR 02-20 01:02:30 engine.py:389]     CacheEngine(self.cache_config, self.model_config,
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/cache_engine.py", line 69, in __init__
ERROR 02-20 01:02:30 engine.py:389]     self.gpu_cache = self._allocate_kv_cache(
ERROR 02-20 01:02:30 engine.py:389]                      ^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:30 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/cache_engine.py", line 103, in _allocate_kv_cache
ERROR 02-20 01:02:30 engine.py:389]     layer_kv_cache = torch.zeros(alloc_shape,
ERROR 02-20 01:02:30 engine.py:389]                      ^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:30 engine.py:389] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 29.25 MiB is free. Process 311009 has 24.24 GiB memory in use. Including non-PyTorch memory, this process has 15.08 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 26.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:30 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 02-20 01:02:30 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Process SpawnProcess-1:
ERROR 02-20 01:02:30 multiproc_worker_utils.py:124] Worker VllmWorkerProcess pid 312479 died, exit code: -15
INFO 02-20 01:02:30 multiproc_worker_utils.py:128] Killing local vLLM worker processes
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/anaconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 391, in run_mp_engine
    raise e
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 380, in run_mp_engine
    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 123, in from_engine_args
    return cls(ipc_path=ipc_path,
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 75, in __init__
    self.engine = LLMEngine(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 276, in __init__
    self._initialize_kv_caches()
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 429, in _initialize_kv_caches
    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 121, in initialize_cache
    self.collective_rpc("initialize_cache",
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 307, in collective_rpc
    return self._run_workers(method, *args, **(kwargs or {}))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 185, in _run_workers
    driver_worker_output = run_method(self.driver_worker, sent_method,
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 306, in initialize_cache
    self._init_cache_engine()
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 312, in _init_cache_engine
    CacheEngine(self.cache_config, self.model_config,
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/cache_engine.py", line 69, in __init__
    self.gpu_cache = self._allocate_kv_cache(
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/cache_engine.py", line 103, in _allocate_kv_cache
    layer_kv_cache = torch.zeros(alloc_shape,
                     ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 29.25 MiB is free. Process 311009 has 24.24 GiB memory in use. Including non-PyTorch memory, this process has 15.08 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 26.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]
INFO 02-20 01:02:30 custom_all_reduce.py:226] Registering 0 cuda graph addresses
[rank0]:[W220 01:02:31.088347103 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Traceback (most recent call last):
  File "/home/hyeznee/.local/bin/vllm", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 204, in main
    args.dispatch_function(args)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 44, in serve
    uvloop.run(run_server(args))
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 875, in run_server
    async with build_async_engine_client(args) as engine_client:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 136, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 230, in build_async_engine_client_from_engine_args
    raise RuntimeError(
RuntimeError: Engine process failed to start. See stack trace for the root cause.
/opt/anaconda3/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/opt/anaconda3/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 02-20 01:02:42 config.py:542] This model supports multiple tasks: {'classify', 'score', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 02-20 01:02:42 config.py:1401] Defaulting to use mp for distributed inference
INFO 02-20 01:02:42 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='snunlp/bigdata_gemma2_9b_dora', speculative_config=None, tokenizer='snunlp/bigdata_gemma2_9b_dora', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=snunlp/bigdata_gemma2_9b_dora, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
WARNING 02-20 01:02:44 multiproc_worker_utils.py:300] Reducing Torch parallelism from 40 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-20 01:02:44 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
INFO 02-20 01:02:45 cuda.py:230] Using Flash Attention backend.
INFO 02-20 01:02:48 __init__.py:190] Automatically detected platform cuda.
[1;36m(VllmWorkerProcess pid=313536)[0;0m INFO 02-20 01:02:49 multiproc_worker_utils.py:229] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=313536)[0;0m INFO 02-20 01:02:50 cuda.py:230] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=313536)[0;0m INFO 02-20 01:02:51 utils.py:950] Found nccl from library libnccl.so.2
INFO 02-20 01:02:51 utils.py:950] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=313536)[0;0m INFO 02-20 01:02:51 pynccl.py:69] vLLM is using nccl==2.21.5
INFO 02-20 01:02:51 pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=313536)[0;0m INFO 02-20 01:02:51 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/hyeznee/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 02-20 01:02:51 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/hyeznee/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 02-20 01:02:51 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_90535da9'), local_subscribe_port=42445, remote_subscribe_port=None)
INFO 02-20 01:02:51 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
[1;36m(VllmWorkerProcess pid=313536)[0;0m INFO 02-20 01:02:51 model_runner.py:1110] Starting to load model snunlp/bigdata_gemma2_9b_dora...
INFO 02-20 01:02:52 weight_utils.py:252] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=313536)[0;0m INFO 02-20 01:02:52 weight_utils.py:252] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[1;36m(VllmWorkerProcess pid=311797)[0;0m INFO 02-20 01:02:52 custom_all_reduce.py:226] Registering 2975 cuda graph addresses
Failed: Cuda error /workspace/csrc/custom_all_reduce.cuh:346 'invalid argument'
ERROR 02-20 01:02:52 engine.py:389] CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 24.27 GiB memory in use. Process 311581 has 15.08 GiB memory in use. Of the allocated memory 23.46 GiB is allocated by PyTorch, and 108.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 02-20 01:02:52 engine.py:389] Traceback (most recent call last):
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 380, in run_mp_engine
ERROR 02-20 01:02:52 engine.py:389]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,
ERROR 02-20 01:02:52 engine.py:389]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 123, in from_engine_args
ERROR 02-20 01:02:52 engine.py:389]     return cls(ipc_path=ipc_path,
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 75, in __init__
ERROR 02-20 01:02:52 engine.py:389]     self.engine = LLMEngine(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 276, in __init__
ERROR 02-20 01:02:52 engine.py:389]     self._initialize_kv_caches()
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 429, in _initialize_kv_caches
ERROR 02-20 01:02:52 engine.py:389]     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 121, in initialize_cache
ERROR 02-20 01:02:52 engine.py:389]     self.collective_rpc("initialize_cache",
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 307, in collective_rpc
ERROR 02-20 01:02:52 engine.py:389]     return self._run_workers(method, *args, **(kwargs or {}))
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 185, in _run_workers
ERROR 02-20 01:02:52 engine.py:389]     driver_worker_output = run_method(self.driver_worker, sent_method,
ERROR 02-20 01:02:52 engine.py:389]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
ERROR 02-20 01:02:52 engine.py:389]     return func(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 307, in initialize_cache
ERROR 02-20 01:02:52 engine.py:389]     self._warm_up_model()
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 337, in _warm_up_model
ERROR 02-20 01:02:52 engine.py:389]     self.model_runner.capture_model(self.gpu_cache)
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
ERROR 02-20 01:02:52 engine.py:389]     return func(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1552, in capture_model
ERROR 02-20 01:02:52 engine.py:389]     graph_runner.capture(**capture_inputs)
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/model_runner.py", line 1903, in capture
ERROR 02-20 01:02:52 engine.py:389]     self.model(
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
ERROR 02-20 01:02:52 engine.py:389]     return self._call_impl(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
ERROR 02-20 01:02:52 engine.py:389]     return forward_call(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 434, in forward
ERROR 02-20 01:02:52 engine.py:389]     hidden_states = self.model(input_ids, positions, kv_caches,
ERROR 02-20 01:02:52 engine.py:389]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 172, in __call__
ERROR 02-20 01:02:52 engine.py:389]     return self.forward(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 305, in forward
ERROR 02-20 01:02:52 engine.py:389]     hidden_states, residual = layer(
ERROR 02-20 01:02:52 engine.py:389]                               ^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
ERROR 02-20 01:02:52 engine.py:389]     return self._call_impl(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
ERROR 02-20 01:02:52 engine.py:389]     return forward_call(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 243, in forward
ERROR 02-20 01:02:52 engine.py:389]     hidden_states = self.mlp(hidden_states)
ERROR 02-20 01:02:52 engine.py:389]                     ^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
ERROR 02-20 01:02:52 engine.py:389]     return self._call_impl(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
ERROR 02-20 01:02:52 engine.py:389]     return forward_call(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/models/gemma2.py", line 81, in forward
ERROR 02-20 01:02:52 engine.py:389]     gate_up, _ = self.gate_up_proj(x)
ERROR 02-20 01:02:52 engine.py:389]                  ^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
ERROR 02-20 01:02:52 engine.py:389]     return self._call_impl(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
ERROR 02-20 01:02:52 engine.py:389]     return forward_call(*args, **kwargs)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 382, in forward
ERROR 02-20 01:02:52 engine.py:389]     output_parallel = self.quant_method.apply(self, input_, bias)
ERROR 02-20 01:02:52 engine.py:389]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py", line 142, in apply
ERROR 02-20 01:02:52 engine.py:389]     return F.linear(x, layer.weight, bias)
ERROR 02-20 01:02:52 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:02:52 engine.py:389] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 24.27 GiB memory in use. Process 311581 has 15.08 GiB memory in use. Of the allocated memory 23.46 GiB is allocated by PyTorch, and 108.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.13it/s]
Traceback (most recent call last):
  File "/home/hyeznee/.local/bin/vllm", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 204, in main
    args.dispatch_function(args)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 44, in serve
    uvloop.run(run_server(args))
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 875, in run_server
    async with build_async_engine_client(args) as engine_client:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 136, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 230, in build_async_engine_client_from_engine_args
    raise RuntimeError(
RuntimeError: Engine process failed to start. See stack trace for the root cause.
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.20s/it]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.02s/it]
/opt/anaconda3/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/opt/anaconda3/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.03s/it]

INFO 02-20 01:02:57 model_runner.py:1115] Loading model weights took 8.6536 GB
[1;36m(VllmWorkerProcess pid=313536)[0;0m INFO 02-20 01:02:58 model_runner.py:1115] Loading model weights took 8.6536 GB
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks.
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242] Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/multiproc_worker_utils.py", line 236, in _run_worker_process
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]     return func(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 231, in determine_num_available_blocks
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]     self._assert_memory_footprint_increased_during_profiling()
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 278, in _assert_memory_footprint_increased_during_profiling
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]     assert self.baseline_snapshot.cuda_memory < cuda_memory, (
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorkerProcess pid=313536)[0;0m ERROR 02-20 01:03:03 multiproc_worker_utils.py:242] AssertionError: Error in memory profiling. Initial used memory 28057796608, currently used memory 10121314304. This happens when the GPU memory was not properly cleaned up before initializing the vLLM instance.
ERROR 02-20 01:03:03 engine.py:389] Error in memory profiling. Initial used memory 26531069952, currently used memory 10125508608. This happens when the GPU memory was not properly cleaned up before initializing the vLLM instance.
ERROR 02-20 01:03:03 engine.py:389] Traceback (most recent call last):
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 380, in run_mp_engine
ERROR 02-20 01:03:03 engine.py:389]     engine = MQLLMEngine.from_engine_args(engine_args=engine_args,
ERROR 02-20 01:03:03 engine.py:389]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 123, in from_engine_args
ERROR 02-20 01:03:03 engine.py:389]     return cls(ipc_path=ipc_path,
ERROR 02-20 01:03:03 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 75, in __init__
ERROR 02-20 01:03:03 engine.py:389]     self.engine = LLMEngine(*args, **kwargs)
ERROR 02-20 01:03:03 engine.py:389]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 276, in __init__
ERROR 02-20 01:03:03 engine.py:389]     self._initialize_kv_caches()
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 416, in _initialize_kv_caches
ERROR 02-20 01:03:03 engine.py:389]     self.model_executor.determine_num_available_blocks())
ERROR 02-20 01:03:03 engine.py:389]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 101, in determine_num_available_blocks
ERROR 02-20 01:03:03 engine.py:389]     results = self.collective_rpc("determine_num_available_blocks")
ERROR 02-20 01:03:03 engine.py:389]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 307, in collective_rpc
ERROR 02-20 01:03:03 engine.py:389]     return self._run_workers(method, *args, **(kwargs or {}))
ERROR 02-20 01:03:03 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 185, in _run_workers
ERROR 02-20 01:03:03 engine.py:389]     driver_worker_output = run_method(self.driver_worker, sent_method,
ERROR 02-20 01:03:03 engine.py:389]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
ERROR 02-20 01:03:03 engine.py:389]     return func(*args, **kwargs)
ERROR 02-20 01:03:03 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
ERROR 02-20 01:03:03 engine.py:389]     return func(*args, **kwargs)
ERROR 02-20 01:03:03 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 231, in determine_num_available_blocks
ERROR 02-20 01:03:03 engine.py:389]     self._assert_memory_footprint_increased_during_profiling()
ERROR 02-20 01:03:03 engine.py:389]   File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 278, in _assert_memory_footprint_increased_during_profiling
ERROR 02-20 01:03:03 engine.py:389]     assert self.baseline_snapshot.cuda_memory < cuda_memory, (
ERROR 02-20 01:03:03 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 02-20 01:03:03 engine.py:389] AssertionError: Error in memory profiling. Initial used memory 26531069952, currently used memory 10125508608. This happens when the GPU memory was not properly cleaned up before initializing the vLLM instance.
Process SpawnProcess-1:
ERROR 02-20 01:03:03 multiproc_worker_utils.py:124] Worker VllmWorkerProcess pid 313536 died, exit code: -15
INFO 02-20 01:03:03 multiproc_worker_utils.py:128] Killing local vLLM worker processes
Traceback (most recent call last):
  File "/opt/anaconda3/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/anaconda3/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 391, in run_mp_engine
    raise e
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 380, in run_mp_engine
    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 123, in from_engine_args
    return cls(ipc_path=ipc_path,
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 75, in __init__
    self.engine = LLMEngine(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 276, in __init__
    self._initialize_kv_caches()
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/engine/llm_engine.py", line 416, in _initialize_kv_caches
    self.model_executor.determine_num_available_blocks())
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 101, in determine_num_available_blocks
    results = self.collective_rpc("determine_num_available_blocks")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/executor_base.py", line 307, in collective_rpc
    return self._run_workers(method, *args, **(kwargs or {}))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/executor/mp_distributed_executor.py", line 185, in _run_workers
    driver_worker_output = run_method(self.driver_worker, sent_method,
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/utils.py", line 2220, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 231, in determine_num_available_blocks
    self._assert_memory_footprint_increased_during_profiling()
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/worker/worker.py", line 278, in _assert_memory_footprint_increased_during_profiling
    assert self.baseline_snapshot.cuda_memory < cuda_memory, (
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Error in memory profiling. Initial used memory 26531069952, currently used memory 10125508608. This happens when the GPU memory was not properly cleaned up before initializing the vLLM instance.
[rank0]:[W220 01:03:04.547990238 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Traceback (most recent call last):
  File "/home/hyeznee/.local/bin/vllm", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 204, in main
    args.dispatch_function(args)
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/scripts.py", line 44, in serve
    uvloop.run(run_server(args))
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 109, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/home/hyeznee/.local/lib/python3.12/site-packages/uvloop/__init__.py", line 61, in wrapper
    return await main
           ^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 875, in run_server
    async with build_async_engine_client(args) as engine_client:
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 136, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/contextlib.py", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/hyeznee/.local/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 230, in build_async_engine_client_from_engine_args
    raise RuntimeError(
RuntimeError: Engine process failed to start. See stack trace for the root cause.
/opt/anaconda3/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/opt/anaconda3/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
