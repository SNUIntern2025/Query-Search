import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from query.query_pipeline import query_pipeline
from search.search_pipeline import search_pipeline
from langchain_huggingface import HuggingFacePipeline
import torch
from langchain_community.llms import VLLM
from final_output import final_output
import argparse
import time
from datetime import datetime
from main import load_model, load_vllm

#ì „ì—­ ë³€ìˆ˜ ì„¤ì •
MODEL_NAME = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"
parser = argparse.ArgumentParser()
parser.add_argument('--vllm', type=str, default='true', help='Using vLLM or not')
args = parser.parse_args()
load_func = load_vllm if args.vllm == 'true' else load_model

try:
    llm = load_func(MODEL_NAME)
except Exception as e:
    print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    raise

with gr.Blocks() as demo:
    #ì œëª© ì„¤ì •
    gr.Markdown("# Chat with a DAG Agent and see its thoughts ğŸ’­")

    chatbot = gr.Chatbot(
        type = "messages",
        label = "Agent",
        height=600
        ) #ì±—ë´‡ ì»´í¬ë„ŒíŠ¸

    with gr.Row():
        msg = gr.Textbox(
            lines=3,
            placeholder="DaG Agentì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.\nex)ì´ë²ˆ ì£¼ í† ìš”ì¼ì— ë¶€ì‚°ìœ¼ë¡œ ì—¬í–‰ì„ ê°€ë ¤ëŠ”ë° ë‚ ì”¨ê°€ ê´œì°®ì„ê¹Œ? ë˜ ê±°ê¸°ì„œ ê°€ë³¼ë§Œí•œ ê³³ ì¶”ì²œí•´ì¤˜",
            scale=9) # ì‚¬ìš©ì ì…ë ¥ ì¹¸
        chat_btn = gr.Button("â¤", scale=1, min_width=50) #ì œì¶œ ë²„íŠ¼

    clear = gr.ClearButton([msg, chatbot]) # ì´ˆê¸°í™” ë²„íŠ¼

    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜: í…ìŠ¤íŠ¸ë°•ìŠ¤ë¥¼ ë¹„ìš°ê³ , ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê¸°ë¡ì— ì¶”ê°€
    def user(user_message, history: list):
        if not user_message.strip():  # ë¹ˆ ë©”ì‹œì§€ ì²´í¬
            return "", history
        return "", history + [{"role": "user", "content": user_message}] 
    
    #ì‘ë‹µ ìƒì„± í•¨ìˆ˜
    def respond(history: list):
        if not history:
            yield history
            return
        try:
            user_message = history[-1]['content']

            subqueries, processed_query = query_pipeline(user_message, MODEL_NAME, llm, args.vllm)

            #ì„œë¸Œì¿¼ë¦¬ ê²°ê³¼ í‘œì‹œ
            history.append({
            "role": "assistant",
            "content": "\n".join([f"- {query}" for query in subqueries]),
            "metadata": {"title": "ğŸ¤” ì§ˆë¬¸ ë¶„í•´ ê²°ê³¼"}
            })
            yield history

            #ë¼ìš°íŒ… ê²°ê³¼ í‘œì‹œ
            routing_result = "\n".join([f"- {query['subquery']}: {query['routing']}" for query in processed_query])
            history.append({
            "role": "assistant",
            "content": routing_result,
            "metadata": {"title": "ğŸ”„ ë¼ìš°íŒ… ê²°ê³¼"}
            })
            yield history

            # ê²€ìƒ‰ ë‹¨ê³„ í‘œì‹œ
            history.append({
            "role": "assistant",
            "content": "ê²€ìƒ‰ ì¤‘...",
            "metadata": {"title": "ğŸ” ê²€ìƒ‰ ì§„í–‰ ì¤‘"}
            })
            yield history

            # ê²€ìƒ‰ ìˆ˜í–‰
            search_result = search_pipeline(processed_query, llm, args.vllm)
            answer = final_output(user_message, search_result, llm)

            #ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
            history.append({"role": "assistant", "content": ""})
            for character in answer:
                history[-1]['content'] += character
                time.sleep(0.05)
                yield history

        except Exception as e:
            print(f"ì—ëŸ¬ ë°œìƒ {e}")
            history.append({
                "role": "assistant",
                "content": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "metadata": {"title": "âŒ ì˜¤ë¥˜ ë°œìƒ"}
            })
            yield history
    
    #ì‚¬ìš©ì ë©”ì„¸ì§€-ì‘ë‹µ ìƒì„± ì²´ì¸ ì„¤ì •
    msg.submit( #Enterí‚¤ë¥¼ ëˆŒë €ì„ ë•Œ
        user,
        [msg, chatbot],
        [msg, chatbot],
        queue = False
    ).then(
        respond,
        chatbot,
        chatbot
    )

    chat_btn.click( #ì œì¶œ ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ
        user,
        [msg, chatbot],
        [msg, chatbot],
        queue = False
    ).then(
        respond,
        chatbot,
        chatbot
    )

    #ì´ˆê¸°í™” ë²„íŠ¼ ì„±ì •
    clear.click(lambda: None, None, chatbot, queue=False)

if __name__ == "__main__":
    demo.launch(share=True)